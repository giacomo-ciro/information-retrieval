# üîçüìñ Information Retrieval
An algorithm to retrieve the most appropriate answer to a given prompt, using discrete embeddings (Track 1), continuous static embeddings (Track 2) or any other embedding strategy (Track 3).

The performance is measured in terms of the BLEU score (n-grams overlapping) between the retrieved response and the true response.

## Methodology
For all the three tracks, I proceeded in similar ways.

Given embeddings for each prompt in the train, dev set and test set, I generated a response to each prompt in the test set by returning the response in the train + dev set whose prompt's embeddings maximized the cosine similarity with the test set prompt's embeddings.

What differs among the three tracks is the way in which embeddings are obtained, which I explain in the following sections.

In neither of the three track I found pre-processing the sentences (punctuation, stop-words etc.) to be useful to improve the final score, hence I never do it in the final methods.

### Track 1
For the first track, I used *TF-IDF* on character tri-grams as discrete embeddings.

After some hyper-parameters search, I found that setting *Minimum document frequency* to 1 (i.e., consider all tri-grams) and *Maximum Document Frequency* to 75% (i.e., discard those tri-gram appearing in more than three quarters of the data, effectively dropping stopwords) yielded the best performance on the dev set, resulting in a BLEU score of 0.090.

### Track 2
For the second track, I trained a *Fast Text* model on the training set to generate sentence embeddings for the training and test prompts.

After some hyper-parameters search, I found that setting *Vector Size* to 300, *Training Epochs* to 100 and *Context Window* to 8 yielded the best performance on the dev set, resulting in a BLEU score of 0.085.

The fact that this score is worse than the one obtained with discrete embeddings is probably due to the performance metric used (retrieved most similar prompt in terms of n-gram overlap is likely to yield better blue score than in terms of semantic similarity). Moreover, the training dataset size is not the issue since I also tried with pre-trained embeddings, which did not improve performance.

### Track 3
For the third track, I used a state-of-the-art *Transformer* model, namely `all-mpnet-base-v2`, to generate sentence embeddings for the training and test prompts.

After testing different models, this one yielded the best performance on the dev set, resulting in a BLEU score of 0.107.

### Appendix: Failed Approaches
In order to conclude the previously reported approaches are the optimal ones, I tried a variety of different methods to find the most relevant response to a given prompt. Eventually, I decided to stick with the ones yielding the highest BLEU score on the dev set. I report here some relevant failed approaches.

- *Word N-Grams*, using both TF-IDF or raw counts.
- *Multiple Character N-Grams*, obtained by concatenating the TF-IDF computed with varying values for n.
- *PCA*, I tried to reduce dimensionality of the discrete embeddings to capture the most relevant directions of change.
- *Doc2Vec*, both a pre-trained version a one trained from scratch on the provided data.  
- *Okapi BM25 Ranking*, which I used in three different ways:
  - to subset the entire set of training prompts into a list of *k* potential *candidates*, and then choose the best one using cosine similarity on the aforementioned discrete or continuous embeddings.
  - to find the most relevant *train prompt*, and return the corresponding response.
  - to find the most relevant *train response*, and return it.
- *Prompt-Response Similarity*, I tried different dynamic embeddings generated by state-of-the-art open source transformer models to find the most appropriate response to a given prompt.
- *Filter Language*, I tried filtering out languages by splitting the dataset into english / non english, and then compute similarity only between same-language prompts.
- *Overfitting*, lastly I tried an approach which directly aimed at generating embeddings such that the most similar vectors were those maximizing the BLEU score.
    
  To do so, I first pair each vector in the dev set with the one in the training set whose respective responses maximize the BLEU score. Then, I generate continuous or discrete embeddings using one of the previously explained method and I fit a matrix Q using gradient descent to maximize the cosine similarity between these pairs, by minimizing the following loss function:
    
  ```
  QÃÇ = arg min_Q -‚àë(i=1 to D) <Qx_i, y_i> / (||Qx_i|| ¬∑ ||y_i||)
  ```
    
  Where x_i, y_i are from the training set and dev set respectively and D is the dev set cardinality.
    
  Intuitively, this matrix should approximate a rotation which aligns as much as possible the vectors maximizing the BLEU score. Unfortunately, this approach yielded sub-optimal results compared to just using the raw embeddings, suggesting there is not enough information in the embeddings to infer the BLEU score.

Surprisingly, none of these methods resulted in a better BLEU score compared to using the simpler yet more effective methods mentioned in the previous sections.

## Results
| Track     | DEV/BLEU    | Notes                                                                                                     | 
| :---:     | :--:        | :--:                                                                                                      |
| 1         | 0.0901      | TF-IDF on char 3-grams, with mininum document frequency = 1                                               |
| 2         | 0.0853      | Fast-Text embedding with subwords of len 2-5, vector size 300, context of 8, trained for 100 epochs       |
| 3         | 0.1070      | sentence transformers embeddings obtained with `all-mpnet-base-v2`                                        |

\* pre-processing was found not to improve performance in any of the tracks.  
\* using similarity between test prompt and train prompt was found to yield better result than similarity between test prompt and train responses directly.  
\* Restricting to k=20 candidates ranked by BM-25 did not improve performance. Increasing k just yielded the same result as not using BM25 at all.  
\* Filtering prompts by language and only matching same-language prompts was found not to influence performance in any direction.

## Usage
Run the main script to retrieve the responses and compute the BLEU score (add `-D` for debugging messages):

```
cd ./src/
python main.py
```

The file `config.json` is the configuration file used to specify the strategy to be used with its hyperparameters, the track to be solved and whether or not we are in testing setting (generate the final .csv file and do not compute BLEU score)

Additional details on the files available are provided:
- `logs.json` a history of configurations tested and their BLEU scores (if some entries have missing keys it means they can be set to anything and won't influence the result)
- `retriever.py` contains the main class with the entire retrieval logic
- 'utils.py' a few functions used in the main script to compute cosine similarity and BLEU scores
- `compute_bleus.py` a script to compute all the cross bleus between dev and train set in parallel and save to a `bleus.npy` file (8+ hours on Intel Xeon)
- `fit_matrix_Q.py` a torch training script to fit the matrix Q as explained in the appendix. It requires the `bleus.npy` file to live in the same directory.

